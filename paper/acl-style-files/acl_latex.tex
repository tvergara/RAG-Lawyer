% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{latex/acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Avoiding Perjury of Large Language Models using Retrieval Augmentation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Tomás Vergara Browne \\
  Pontificia Universidad Católica de Chile \\
  \texttt{tomvergara@uc.cl} \\}

\begin{document}
\maketitle
\begin{abstract}
  Retrieval Augmentation provides the abilities to Large Language Models to access external information for them to generate more factual answers. Although the use of Retrieval Augmentation techniques has been intensely studied in the last year, a particular domain of legal domains has not been significantly studied, in comparison to other domains. In this paper, we explore the use of Retrieval Augmentation for the Mistral 7B model, which has surprised the open source community for its ability to supersede in performance models which are considerably larger. Through the use of the open source framework of RETA-LLM, we are able to add retrieval capabilities to the model. With this model, we are able to provide a new state of the art for the benchmark of LexGLUE.
\end{abstract}

\section{Introduction}
Retrieval Augmented Generation (RAG) is a technique to combine information retrieval, with a text generator. I has been extensively studied over this last year the use of RAG with Large Language Models (LLMs), showing that it can substantially improve a model's performance \cite{ram2023context}. One particular aspect that this technique is allowing to improve is the models factuallity of its answers \cite{lewis2020retrieval}.

Altough there has been some exploration on the use of LLMs in the legal domain \cite{chalkidis2020legal}, there has not been that much focus on the use of RAG techniques to enhance a model's abilities to perform on standarized legal benchmarks, such as LexGLUE \cite{chalkidis-etal-2022-lexglue}. For example, \cite{savelka2023explaining} uses a RAG technique with GPT-4 and empirically shows that it makes the model less prone to hallucinations and provides more factual answers when explaining legal concepts.

Also, the model Mistral 7B \cite{jiang2023mistral} has arised as a potential state of the art in relatively small open source LLMs. It is able to outperform much bigger models in many domains, such as Llama 2 with 30B \cite{touvron2023llama}.

This calls for the natural approach of combining RAG and Mistral 7B, into the LexGLUE benchmark. Considering the advances that this model and RAG can make in performance, it is not unrealistic to aim for a new state of the art in the LexGLUE benchmark.
\section{Related Work}
\subsection{Fine tuning large language models in the legal domain}
\subsection{Retrieval augmented large language models in the legal domain}

\section{Methods}
We are using the open source framework of RETA-LLM \cite{liu2023reta} to enhance Mistral 7B with RAG capacities. This framework allows as to do a "\textit{plug-and-play}" approach, in which there is no need for fine-tuning the model to know how to use the external memory.

For our external knowledge base, we are going to use data from the US and European Law, given that the LexGLUE benchmark is based in legal cases from the US and countries within Europe.


\section{Results}

\section{Discussion}


\section{Appendices}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}

\appendix


\end{document}
